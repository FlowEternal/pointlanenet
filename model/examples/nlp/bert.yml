pipeline: [fully_train]

fully_train:
    pipe_step:
        type: TrainPipeStep
    dataset:
        type: GlueDataset
        common:
            task_name: mrpc
            batch_size: 32
            data_path: /cache/datasets/MRPC/
            vocab_file: /cache/nlp/bert-base-uncased/vocab.txt
    trainer:
        type: Trainer
        epochs: 3
        model_statistics: False
        optimizer:
            type: AdamW
            params:
                lr: 0.00001
        loss:
            type: CrossEntropyLoss
        metric:
            type: NlpMetrics
    model:
        pretrained_model_file: /cache/nlp/bert-base-uncased/pytorch_model.bin
        head: model.cls
        model_desc:
            type: BertClassification
            config: {
                "attention_probs_dropout_prob": 0.1,
                "hidden_act": "gelu",
                "hidden_dropout_prob": 0.1,
                "hidden_size": 768,
                "initializer_range": 0.02,
                "intermediate_size": 3072,
                "layer_norm_eps": !!float 1e-12,
                "max_position_embeddings": 512,
                "model_type": "bert",
                "num_attention_heads": 12,
                "num_hidden_layers": 12,
                "pad_token_id": 0,
                "type_vocab_size": 2,
                "vocab_size": 30522
            }



